\chapter{Building a Lua-based configuration-driven processor simulation}
\label{ch:lua-config}

\newcommand{\cmakeinline}[1]{\mintinline{CMake}{#1}}
    
After concluding work on procsim.scala (\cref{ch:scala-akka}), this author considered taking the lessons learned from the Scala project and attempted to apply them to the original hc12sim project \cite{Brightwell2013}. Unfortunately, the hc12sim simulator was dated, poorly designed and poorly organized due to inexperienced student work. A large portion of effort was placed in modernizing and correcting the hc12sim software to be well tested, build in a cross-platform environment and ease development and maintenance. Following these improvements, the software was renamed procsim as it was intended to be a general purpose simulation framework rather than tied to the \hcmodel{}. From working with Scala-based configurations in \cref{sec:sec:procsim-scala:configuration}, the author believed that runtime-configuration was extremely important in defining processors for students. The author chose to expand the hc12sim project and replace the JSON-based, compile-time configuration capabilities with runtime-based configurations utilizing an embedded scripting language. Within the C++ programming ecosystem, there exists many different scripting environments that can be embedded with varying degrees of difficulty, feature capabilities and execution speeds. The author investigated utilization of several scripting environments before settling on utilizing Lua \cite{Lua:Homepage} with bindings provided through sol2, a wrapper between C++ and Lua API calls \cite{Github:ThePhD:sol2}. Once Lua was selected, the author designed configuration schemas built to execute in a Lua sandbox with timing and other hardware-level considerations abstracted away from the configuration definitions as much as possible.

\section{Creating procsim: Modernizing hc12sim}

The hc12sim project that was showcased in \cite{Brightwell2013} was written to be modern at the time. Since 2013, the C++ community has progressed thoroughly and many of the libraries in use within the project were discontinued or lost support. The hc12sim project utilized many poor practices such as excessive use of \cxxinline{typedef} statements making it very difficult to reason what parts were. Additionally, the project heavily relied on Boost's non-standard implementations of \verb|tr1| structures such as \cxxinline{boost::shared_ptr<T>} \cite{Boost1.53.0:SmartPointers} (now superseded by \cxxinline{std::shared_ptr<T>} \cite{cppreference:shared-ptr}) and the C++11 threading in Boost \cite{Boost1.53.0:Thread} (superseded by \cxxinline{<thread>} \cite{cppreference:thread}). The insidious nature of threading libraries and smart pointer usage made this effort an unfortunately large task. Additionally, when writing C++ software distribution of binary files is a common concern when working in cross-platform environments. Thus, a concious effort was made to remove all binary-based libraries in favour of header-only library usage which remove most concerns of distributing incompatible binaries.


\subsection{Cross-platform development}
\label{sec:lua:sec:cross-platform-development}

The C++ language has unofficially always tried to meet the motto: ``write once, compile anywhere.'' With any compiled language, when moving between platforms it can be non-trivial to assure that compiled code will run between any two platforms. Given that \cref{req:personal} requires that any such solution must run within personal computers, the author was required to have the procsim project run on the three major platforms, Microsoft Windows, macOS and GNU/Linux. Within the C++ ecosystem, the most assured way to complete this is to try and utilize only commonly used tools and try to utilize the Standard Template Library (STL) as much as possible. That is not to say there are not powerful cross platform libraries (e.g. Qt \cite{Qt}, Boost \cite{Boost}), these libraries provide excellent tools but often introduce complex build requirements and add large binary files into any distributed application. An artefact of 2013, Microsoft's Visual C++ (MSVC) 2011 compiler and accompanying STL were not completely standards compliant with C++11 and thus \cite{Brightwell2013} required the use of Boost \cite{Microsoft:MSVC:ModernCPP:2011}. Many of the C++11 STL additions were modelled after Boost's implementations \cite{Meyers2005}. In 2015, the Boost's STL implementation dependency was removed by upgrading the lowest supported compiler to MSVC 2015 which fully supports C++11 in both compiler semantics and STL compatibility \cite{Microsoft:MSVC:ModernCPP}. This allowed the author to fully remove the main Boost dependencies on Boost.Memory and Boost.Thread. By removing these library dependencies, it eased cross-platform development as only the Boost C++ headers were required to compile the procsim project. However, the unit test platform for hc12sim was based on Boost.Test \cite{Boost1.53.0:Test} -- a problem addressed later.

\subsubsection{Access to multiple platforms}

Once the procsim project utilized the C++11 standard STL over Boost the next task was configuring mechanisms to ensure platforms performed correctly. It was found multiple times that different compiler's STLs behaved differently depending on different versions. For example, the author found that the GNU Compiler Collection (GCC) \cite{GCC} 5.X C++ STL shipped with a broken version of the C++11 \cxxinline{<regex>} library. This bug was found late because testing on macOS's Xcode 8.3 and MSVC 2015 passed (the authors two main platforms). However, on a ``whim'' building on Linux was tested and the issue was found -- unit tests checked this functionality. This small issue opened up a question: How can the software be verified working on three major platforms while not having native access to each platform? The approach at the time was to change computers to access the different major operating systems. It became difficult to maintain what changes were made over time to have the project build properly. This meant that sharing the software development environment became difficult and near impossible to replicate at times. In previous works, the author had utilized VMWare vSphere \cite{VMWare:vSphere} technologies to create common developer ``environments'' through Virtual Machines that could be consistently used for development and testing. 

Given that vSphere is a proprietary and expensive software, the author researched other tools that provided virtualized environments. Oracle's VirtualBox \cite{Oracle:VirtualBox} provides virtual machine provisioning through open source tools. Building on the idea of ``cloning'' common configurations between development environments through VirtualBox, the author eventually found HashiCorp's Vagrant \cite{VagrantUp}. Vagrant provides a consistent declarative interface for defining a virtual machines configuration through Ruby scripts. Vagrant's parent company, Hashicorp, hosts ``boxes'' that are preconfigured virtual machines ranging from Ubuntu Linux to FreeBSD to macOS El Capitain \cite{Vagrant:Boxes}. These ``base'' boxes are then configured to match the environment requirements for a project. For example, \cref{lst:lua:ubuntu-vagrant-box} show cases a simple configuration that installs both GCC 5.X and Low-Level Virtual Machine (LLVM) Clang \cite{LLVM:Clang} 3.7 -- two fully C++11 compliant compiler tool-chains. These configurations are known as ``Vagrantfiles'' and are full Ruby programs, implying they can utilize Ruby's full ecosystem of software. The author used Vagrant and VirtualBox to create three environments for Microsoft Windows 10, macOS El Capitain, Ubuntu 14.04 Precise and Ubuntu 16.04 Xenial. These configurations allowed instantiating virtual machines quickly and consistently on any platform to build and run test configurations. With the three platforms easily accessible, it became necessary to improve the build infrastructure to be consistent across these platforms.

\begin{listing}[tp]
\begin{minted}[breaklines=true]{Ruby}
Vagrant.configure("2") do |config|
    # Utilize the ubuntu/trusty64 box to give a 14.04 environment
    config.vm.box = "ubuntu/trusty64"
    
    config.vm.provider "virtualbox" do |vb|
        vb.cpus = 2
        vb.memory = 2048
    end
    
    # Install/update prerequisite software:
    config.vm.provision "shell", privileged: true, inline: <<-EOF
        echo "Installing build prequisisites: git, compiler ppas"
        apt-get install -q -y git 
        # Compiler tool-chains
        apt-add-repository -y ppa:ubuntu-toolchain-r/test 
        apt-add-repository -y ppa:adrozdoff/llvm-backport 
        apt-get update -qq
    EOF
    
    config.vm.provision "shell", privileged: true, inline: <<-EOF
        echo "Installing build compiler toolchains"
        apt-get install -q -y clang-3.7 \
                              gcc-5 g++-5 \
                              --force-yes
    EOF
    
    # Update all existing packages last
    config.vm.provision "shell", privileged: true, inline: "apt-get dist-upgrade -y ; apt-get autoremove --purge -y"
end # end vagrant file 
\end{minted}
\caption{Vagrant file that describes a Ubuntu 14.04 box with modern GCC 5.X and LLVM Clang 3.7 compilers installed.}
\label{lst:lua:ubuntu-vagrant-box}
\end{listing} 

\subsubsection{Building with cross-platform tools}

CMake \cite{Kitware:CMake} is a cross-platform meta-build system. A meta-build system provides a ``front-end'' language for defining a project's build and then generates a build program for other build systems to consume. CMake provides support for many different build systems, each is supported through a CMake ``generator''. The generators required to build on the Windows, macOS and Linux were ``Unix Makefiles'' for GNU \verb|make| \cite{GNU:Make} on Linux and macOS, ``Xcode'' for macOS's IDE Xcode \cite{Apple:Xcode}, and ``Visual Studio 14 2015 [Win64]'' for MSVC 2015 \cite{CMake:Generators}. On Unix environments, \verb|make| is generally available however and a newer build tool, \verb|ninja| \cite{NinjaBuild}, is preferred as builds are faster and provides the same familiar behaviours as \verb|make|. As stated, CMake provides a cross-platform meta-build system, however this meant writing cross-platform build scripts for the procsim project. In the original hc12sim project \cite{Brightwell2013}, CMake was utilized however it was poorly organized and added large coupling file dependencies. For example, changing a single file caused the entire build to trigger rather than just dependencies. At first, this was thought to be caused by CMake not properly structuring file dependencies, however it was soon found to be due to poor header isolation and deep, nested coupling. The author separated all headers to try and isolate concerns of what a header ``does'' and remove inter-dependencies as much as possible. Reorganization of the procsim library coupled with improved build tools and the addition of OBJECT library files \cite{CMake:add_library} significantly reduced build time and allowed for less time spent compiling the procsim project. 

As previously discussed, compilers come with consequential differences in support for new and current standards. In order to try and utilize features appropriately it is often helpful to ``test'' compilers support of features. CMake provides this functionality through cmake-compiler-features \cite{CMake:compile-features} but CMake's built-in support lags heavily behind current C++ standards (including C++14 and 17 at the time of writing). Fortunately, compatibility \cite{CMakeCompatibility} is a library used to enhance compiler and STL feature detection. The author worked with M\"uller to improve compatibility features and generation of header files that will include a ``shim'' on top of the current STL to add features where possible. ``Shims'' can only be added for STL features as syntax elements are not extensible in C++. The use of compatibility allows the procsim project to build more easily on the three major platforms. 

\subsection{Testing Infrastructure}

Testing a multi-platform compiled project is very difficult and relies on many tools to be successful. Through utilization of Vagrant and better build infrastructure within CMake scripts, the procsim project was able to be automatically tested on multiple platforms through continuous integration software. Continuous integration describes a process of building, testing and deploying software as rapidly as possible to reduce time-to-market for software projects \cite{Stolberg2009}. Idealistically, by utilizing continuous integration, developers receive near-instantaneous feedback on whether or not a particular change set breaks the product in unforeseen ways -- this heavily depends on the level of testing and environments available. For procsim, the author configured a small ``micro-PC'' with an Intel \textregistered{} i5-4590T and 4 GB of RAM to run a Jenkins \cite{Jenkins:Home} continuous integration server. 

Within Jenkins, Vagrant was used to create virtual machines that acted as ``slaves'' to the ``master'' Jenkins service utilizing Jenkins' distributed build infrastructure \cite{Jenkins:DistributedBuilds}. Each of these virtual machines was spawned to utilize a separate configuration to test the procsim project against. For example, one machine spawned a Ubuntu 14.04 machine with manually installed compiler tool-chains and a second machine spawned a macOS instance with Xcode 8. Each machine ran the exact same unit tests after configuration of the virtual machine and selection of a CMake generator. This was achieved utilizing a Jenkins Pipeline \cite{Jenkins:Pipeline} specified in a Jenkinsfile script \cite{Jenkins:Pipeline:Jenkinsfile}. Jenkins Pipelines allow for parallel execution of ``tasks,'' dependent and independent, for a continuous integration task graph. For the procsim project, due to its immaturity, the pipeline only included stages for building the software and running unit tests within the multiple operating system platforms required by \cref{req:personal}. While not strictly related to simulation \cref{req:simulations}, the addition of a continuous integration solution allowed the author to maintain a rapid pace of development and provide confidence that changes made worked across all platforms.  

Over the course of the procsim project, the build process became more and more complicated requiring improvements in automating the build process. When writing unit tests, the original hc12sim project utilized Boost.Test \cite{Boost1.53.0:Test} for creating and running unit tests. Unfortunately, Boost.Test required library distribution which made working on platforms without software distribution systems difficult (e.g. Windows or macOS). In order to remove the final thread of Boost binary dependence, the author replaced Boost.Test with Catch \cite{CatchLib}. Catch is a header-only C++ testing framework built on top of modern C++. The single-header configuration removed the final binary dependency. Catch provides a Behaviour-Driven Development (BDD) interface that allows developers to describe the behaviour of their system within their tests creating easily understandable and clearly organized tests \cite{Solis2011}. Catch improved the mechanisms used to test and provided an easier to maintain testing suite. Catch's test runner requires that all tests are defined through C-style macros and the executable must include Catch's main through \verb|main| the \cxxinline{CATCH_CONFIG_MAIN} macro \cite{CatchLib:Tutorial}. When defining many tests, it is often useful to organize them in meaningful ``suites'' of test executables. With any testing framework, when creating multiple executables it is advantageous to share object resources to reduce compiling time -- particularly in \cxxinline{template} or \cxxinline{constexpr} heavy software.

\subsection{Improving developer work flows within CMake}

In \cref{sec:lua:sec:cross-platform-development}, CMake was discussed as the build tool for the procsim project. CMake utilizes \cmakeinline{OBJECT} libraries to allow multiple ``targets'' to share compiled object files and statically link against the same objects as required. These \cmakeinline{OBJECT} libraries are not the same as shared objects or DLLs and should not be treated as the same. An \cmakeinline{OBJECT} library is a CMake abstraction on top of compiled compilation units that must be statically linked into some other object. These shared \cmakeinline{OBJECT} libraries were very useful in unit test files as a common object used between each executable ``suite'' is the \verb|main| test runner. Manually specifying all of these suite instances to get individual suites and manually maintain a global ``test all'' suite was frustrating. The author developed a series of CMake macros \cite{CMake:macro} and functions \cite{CMake:function} to reduce the required commands to define tests and library files. \Cref{lst:lua:cmake-test-declaration} shows a simple shared library defined utilizing the build infrastructure described. Three functions are used to define four distinct targets: 
\begin{enumerate}
    \item \cmakeinline{new_project()} defines a ``project'' of sources which could be an executable, shared/static library, etc.
    \item A library, \verb|procsim| with headers, sources and external dependencies;
    \item Two test suites that test ``packages'' of code in the library from (1) and depend on it and a common test-harness;
    \item An aggregate \cmakeinline{PROJECT} suite that combines all the tests from each suite into the current project
\end{enumerate}
Within these functions, each performs many tasks to try and simplify definition of parts of a completed program.

\begin{listing}[hp]
\begin{minted}{CMake}
new_project()

# Define a new library called ``procsim''
new_library(procsim 
    HEADERS procsim/encoding/Algorithm.hpp
            procsim/encoding/Code.hpp
            procsim/encoding/Operand.hpp
            procsim/encoding/Primitives.hpp
            procsim/encoding/Utility.hpp
            
            procsim/time/Clock.hpp
            procsim/time/Timer.hpp
            procsim/time/AsyncTimerReceiver.hpp
    
    SOURCES encoding/Algorithm.cpp
            encoding/Code.cpp
            encoding/Operand.cpp
            encoding/Primitives.cpp
    
            time/Clock.cpp
            time/Timer.cpp
            time/AsyncTimerReceiver.cpp
    
    GENERATED_HEADERS ${PROCSIM_EXPORT_HEADER}
                      ${PROCSIM_COMPILER_HEADER}
    VENDORS cpp17_libs fmtlib                      # non-builtin
    INCLUDE_DIRECTORIES ${Boost_INCLUDE_DIRS}
    LIBS                ${CMAKE_THREAD_LIBS_INIT}) # builtin libs

# Create two test suites utilizing the new library      
create_test(time     SOURCES time/ClockTest.cpp 
                             time/TimerTests.cpp
                     LIBS    procsim test-harness)
                     
create_test(encoding SOURCES encoding/CodeTest.cpp
                             encoding/OperandTest.cpp
                             encoding/PrimitivesTest.cpp
                             encoding/UtilityTest.cpp
                     LIBS    procsim test-harness)
# Create a test for the whole project
create_test(core     PROJECT)
\end{minted}
\caption{CMake script showing how two test suites, \cmakeinline{time} and \cmakeinline{encoding}, are defined as part of a project, \cmakeinline{core}}
\label{lst:lua:cmake-test-declaration}
\end{listing}
 
The function, \cmakeinline{new_project()} defines a set of many ``global'' variables within the build that are named after the directory that the current defined CMake script is found. For example, if \cref{lst:lua:cmake-test-declaration} was found in ``procsim/core'' the project would be called ``core.'' This behaviour is meant to be consistent and simplistic. The model is based on the idea of ``convention over configuration'' which was popularized by tools and frameworks like Apache Maven \cite{Maven:StandardDirectoryLayout} and Ruby on Rails \cite{Heinemeier:RailsDoctrine}. The function defines the following global state (truncated for brevity): 
\begin{itemize}
    \item \cmakeinline{SOURCE_DIR}: Implementation files
    \item \cmakeinline{INCLUDE_DIR}: Header files
    \item \cmakeinline{TEST_DIR}: Test implementations
\end{itemize}
These properties are used to generate ``namespaced'' build targets and variables allowing for use in specification within CMake's dependency graph. To simplify, if you define two projects, \verb|a| and \verb|b|, you can set \verb|b| to depend on \verb|a| being built first. Once the \cmakeinline{new_library()} function is called within a cmake script hierarchy, all further functions from the build tools will utilize the state set by the project. 

The function \cmakeinline{new_library()} creates a new library target that compiles all of the specified files to create a library. The function accepts \cmakeinline{OBJECT}, \cmakeinline{INTERFACE}, etc. to match \cite{CMake:add_library} trying to remove any discontinuity between the two functions. As shown in \cref{lst:lua:cmake-test-declaration}, there are several other parameters used: 
\begin{description}
    \item[HEADERS] Header files
    \item[SOURCES] Implementation files
    \item[GENERATED\_SOURCES] Source files that are generated at build time by another target
    \item[GENERATED\_HEADERS] Same as \verb|GENERATED_SOURCES| except Headers
    \item[VENDORS] These are external dependencies that are built within the build chain after downloading from an external source. This forces the library to depend on their download and build
    \item[LIBS] These are internal CMake libraries or other targets that are built in a multi-project configuration
\end{description}
Utilizing these arguments, the function creates appropriate CMake targets and adds the required dependencies to the new library target. In addition, it performs platform-specific compiler adjustments to try and reduce the amount of configurations required per target. For example, many flags for MSVC to not apply to Clang which do not apply to GCC. This torrent of compiler option configuration is eased by the use of CMake's generator expressions \cite{CMake:generator-expressions} but many compilers do not properly identify with CMake's infrastructure so manual adjustments must be made to modernize these facilities. Further, by utilizing our own mechanism, the targets created have ``localised'' include directories that do not pollute other targets. If two targets require different versions of the same files, the built-in \cmakeinline{include_directories()} command will cause these targets to ``collide'' and fail to compile \cite{CMake:include-directories}. Further, the \cmakeinline{new_library()} appropriately applies the \cmakeinline{SYSTEM} option to all includes that are outside of the whole multi-project build so any warnings generated are not applied. Lastly, \cmakeinline{install} targets are created that will install headers and compiled binaries to appropriate locations depending on the type of library and the platform in use (Microsoft Windows requires libraries be beside their executables, where UNIX utilizes PATH resolution mechanics).

The last function used in \cref{lst:lua:cmake-test-declaration} is \cmakeinline{create_test()} which has three distinct incantations. The first creates a single test suite from Catch-based test sources (\cite{CatchLib}). These sources are aggregated and compiled to an \cmakeinline{OBJECT} library. This library is concatenated to the current \cmakeinline{PROJECT}'s test executable. By generating \cmakeinline{OBJECT} libraries, these smaller units of compilation can be combined and reused intelligently saving large amounts of compile time as previously discussed. The second use of \cmakeinline{create_test(test PROJECT)} generates a \cmakeinline{PROJECT}-based test executable and ``check'' target that runs the executable. This test executable includes all previously defined \cmakeinline{OBJECT} libraries in the current ``project.'' This executable is used to debug and execute tests for the given project, it is currently not possible to run a smaller test individually. Additionally, the generated executable is registered with CMake's CTest which creates build-script targets to run test executables providing dashboards and other useful tools \cite{CMake:CTest}. The last use case is \cmakeinline{create_test(target ALL)}. The \cmakeinline{ALL} request generates a test executable that includes every test suite created in a multi-project build into a single executable -- a convenience executable. As multi-project programs develop, it is useful to be more and less granular depending on the context at the time of use. While intuitively it appears as though large amounts of executables create large amounts of compilation units creating a very slow build time, due to the use of \cmakeinline{OBJECT} libraries created at definition, adding more test executables only adds incremental link time due to linking static objects -- a marginal increase of time for the convenience provided. The full test output from the procsim project is shown in \cref{lst:lua:ctest-output}. The output is easy to read and should errors occur, catch provides assertion errors and suites will continue summarizing errors later.

\begin{listing}[hb!]
\begin{verbatim}
1>--- Build started: Project: RUN_TESTS, Configuration: Debug x64 ---
1>  Test project S:/research/procsim/build-windows
1>        Start  1: core-components
1>   1/11 Test  #1: core-components ............   Passed    0.09 sec
1>        Start  2: core-encoding
1>   2/11 Test  #2: core-encoding ..............   Passed    0.06 sec
1>        Start  3: core-time
1>   3/11 Test  #3: core-time ..................   Passed    4.46 sec
1>        Start  4: core-misc
1>   4/11 Test  #4: core-misc ..................   Passed    0.09 sec
1>        Start  5: conf-encoding
1>   5/11 Test  #5: conf-encoding ..............   Passed    0.20 sec
1>        Start  6: conf-loader
1>   6/11 Test  #6: conf-loader ................   Passed    0.06 sec
1>        Start  7: conf-arch
1>   7/11 Test  #7: conf-arch ..................   Passed    0.07 sec
1>        Start  8: conf-components
1>   8/11 Test  #8: conf-components ............   Passed    0.13 sec
1>        Start  9: conf-proc
1>   9/11 Test  #9: conf-proc ..................   Passed    0.18 sec
1>        Start 10: conf-time
1>  10/11 Test #10: conf-time ..................   Passed    0.07 sec
1>        Start 11: conf-env
1>  11/11 Test #11: conf-env ...................   Passed    0.04 sec
1>
1>  100% tests passed, 0 tests failed out of 11
1>
1>  Total Test time (real) =   5.50 sec
======= Build: 1 succeeded, 0 failed, 1 up-to-date, 0 skipped =======
\end{verbatim}
\caption{Test output from CTest \cite{CMake:CTest} from Microsoft Visual Studio Community 2015 for the procsim project.}
\label{lst:lua:ctest-output}
\end{listing}  

All of these build improvements worked towards improving software maintainability of the simulation suite. Whilst these changes do not directly contribute to the \cref{sec:problem-statement}, it is patent that improvements in developer work flow have a direct consequence in improvements to developer time resources and reduces the likelihood of poor software being produced. Given improvements to developer productivity, one may argue simply that this improves the validity of producing modern software (\cref{req:modern}) and any software produced will more accurately meet requirements specified. As such, these improvements allowed this author to develop the procsim project more rapidly and reduced time spent fighting against compilers and cross-platform development issues.


\section{Utilizing runtime configurations through scripting}

procsim.scala had a VHDL-like syntax for defining instructions within a CPU simulation. Providing the same facilities within a non-managed language like C++ is not possible without the use of a scripting engine. When researching possible solutions to this problem, several scripting languages stood out: Python \cite{Python:Homepage}, JavaScript through Google's V8 \cite{Google:V8} or Mozilla's SpiderMonkey \cite{MDN:SpiderMonkey}, Lua \cite{Lua:Homepage}, or reusing Scala through the Java Native Interface (JNI) \cite{Oracle:JNI}. In addition to native tools provided for integration by each of the scripting languages listed, Simple Wrapper and Interface Generator (SWIG) \cite{SWIG:Homepage} provides a cross-language framework for wrapping software to multiple scripting engines. 


\subsection{Scripting language selection}

Previously, several scripting languages were evaluated against each other for candidacy as the configuration specification language. First, Python \cite{Python:Homepage} was considered. Python is a mature, stable, dynamically typed language with wide use in the industry, scientific and academic communities \cite{StackOverflowSurvey2016}. Python was considered due to it's exposed foreign function interface (FFI) interface which allows for C functions to be called from Python or C functions to call into Python. Python's language supports overriding operators \cite{Python:Operators}, object-oriented programming \cite{Python:Classes} and low-level bitwise operations \cite{Python:BuiltinTypes}. These features are all excellent to have in implementing low-level hardware simulations and configurations. For working with the Python engine, the Python community provides a library called CFFI that utilizes python's FFI interface to call into and from Python. CFFI's purpose is to provide a bridge layer between Python and C. Any library wishing to utilize CFFI for interacting with the Python engine requires all functions be exposed as C functions. The procsim project was written utilizing C++11 syntax and does not lend well to a C-based API. Thus utilizing CFFI would become tedious for this implementation. An alternate to the CFFI library is Boost.Python \cite{Boost1.53.0:Python} which provides binding mechanisms for working with complex classes in C++ and having them work within the Python virtual machine. Boost.Python provides a very simple and easy to use syntax for defining types. Further, Boost.Python respects \cxxinline{constructor} and \cxxinline{destructor} semantics of any types passed into Python. \Cref{lst:lua:python-example} showcases a simple type exposed into Python. Given the simplicity of exposing data into Python, and the ubiquitous nature of the language, it lends itself well to a pedagogical application (\cref{req:pedagogical}) and will feel modern to students coming from Software Engineering contexts (\cref{req:modern}). Unfortunately, the largest compelling argument against Python is that to run Python scripts, one must install a Python virtual machine adding an extra dependency that is external to the project. This makes distribution more difficult for personal computers, though not all as some operating systems come with python pre-installed (e.g. most Linux distributions and macOS).

\begin{listing}[hp!]
\begin{minted}{C++}
// Simple Structure
struct World
{
    World(std::string msg): msg(msg) {} 
    void set(std::string msg) { this->msg = msg; }
    std::string greet() { return msg; }
    std::string msg;
};
\end{minted}

\begin{minted}{C++}
#include <boost/python.hpp>
using namespace boost::python;

// Define a python module
BOOST_PYTHON_MODULE(hello)
{
    class_<World>("World", init<std::string>())
        .def("greet", &World::greet)
        .def("set", &World::set)
        ;
}
\end{minted}

\begin{minted}{python}
>>> import hello
>>> planet = hello.World('hello')
>>> planet.greet()
'hello'
>>> planet.set('howdy')
>>> planet.greet()
'howdy'
\end{minted}
\caption{Example of exposing a C++ class to Python \cite{Boost1.53.0:Python}.}
\label{lst:lua:python-example}
\end{listing}

JavaScript is by far the most popular language in use in modern applications \cite{StackOverflowSurvey2016}. JavaScript provides many of the same features as Python, but does not have as ``strong'' of a type system. JavaScript provides many coercions of types into other types making it difficult to reason at times compared to Python (e.g. \mintinline{JavaScript}{{} + [] === 0} for reasons outside the scope of this document). While providing a modern interface for students (\cref{req:modern}), it also creates a regrettable pedagogical experience due to the extremely high-level nature of the language for low-level implementations (\cref{req:pedagogical}). In order to embed JavaScript within an application, it requires a JavaScript Virtual Machine to be embedded. The two current leading JavaScript engines are Google's V8 \cite{Google:V8} and Mozilla's SpiderMonkey \cite{MDN:SpiderMonkey}. SpiderMonkey provides a C++ API to embed functions and values into the engine. This C++ API is very similar to the API provided by Python's CFFI, however it provides slightly stronger type safety than a traditional C API utilizing \cxxinline{void*} arguments. When looking to integrate the SpiderMonkey VM into procsim, one of the unforunate consequences of the library is that the entire virtual machine is bound to a single thread of execution \cite{MDN:SpiderMonkey:UserGuide}. JavaScript itself is a single-threaded language which is largely inconsequential for configuration declaration. Though due to SpiderMonkey itself being bound to a single thread, it may become too difficult to efficiently produce threaded software when accessing SpiderMonkey. The most commonly used JavaScript engine available is Google's V8 engine \cite{Google:V8} as it powers Node.js, Chromium and Electron-based applications. Embedding V8 is a difficult task as it requires more code calls to bind components and building the library itself is complicated \cite{Google:V8:Embedding}. To have users utilize v8 within procsim, the entire engine would need to be shipped with the application. Providing a working ``drop-in'' source for v8 or SpiderMonkey is unfortunately not feasible. As such, utilizing v8 or SpiderMonkey is not feasible for this project as an embedded scripting language.

Java and Scala both require the Java Virtual Machine to execute software. This massive requirement is not one to be taken lightly as it is several hundred mebibytes in size. The DSL created within the procsim.scala project utilized a VHDL-like syntax that this author believed to be beneficial to reducing the pedagogical load on students. In order to access Scala software in an embedded context, software wrappers must be written to utilize the Java Native Interface (JNI) \cite{Oracle:JNI}. The JNI is notoriously frustrating to write software for because C/C++ is not a managed language and the JVM is a managed VM making memory guarantees frustrating to correctly implement. Simplified Wrapper and Interface Generator (SWIG) provides an excellent wrapper definition tool to generate bindings for C++ classes into the JNI \cite{SWIG:Homepage}. SWIG allows developers to write generator files using a C++-like syntax which includes extra meta-information that is added to appease the Java Virtual Machine. SWIG uses ``directives'' to annotate existing C/C++ code to generate efficient JNI code that can then be compiled into a C/C++ application as any other software \cite{SWIG:Java}. SWIG does not support method references which could prove problematic when implementing. In addition, working with SWIG wrappers it non-trivial as classes become more complicated. SWIG has not updated over time to keep up with C++ standards and does not directly support many semantics such as \cxxinline{std::unique_ptr} and method references. With the additional overhead of utilizing the JVM, reusing the older procsim.scala DSL is not feasible as an embedded scripting language -- Java is not meant to be embedded.

Lastly, the Lua programming language was considered as it is by design an embedded scripting language \cite{Lua:Homepage}. The Lua community directly states: \textquote[\cite{Lua:Homepage}]{Lua is a powerful, efficient, lightweight, embeddable scripting language. It supports procedural programming, object-oriented programming, functional programming, data-driven programming, and data description.} These features match the features previously defined by Python. Lua puts data description as a first order citizen allowing for extremely descriptive dictionaries of heterogeneous data \cite{Ierusalimschy:PIL}. Similar to JavaScript's Object, Lua provides traditional object-oriented programming through an associative array known as a Table \cite{Ierusalimschy:PIL}. \Cref{lst:lua:project-example} shows a simple dictionary-like description of a ``project entry'' that the \url{https://lua.org} site uses to display known projects. Creating instances like this in Lua is trivial and through libraries like tableshape \cite{Github:leafto:tableshape} add type checking to make sure a table will conform to a schema or ``shape.'' Interacting with Lua's VM is done through direct calls to Lua's C API or by utilizing libraries or generators. SWIG provides a poor wrapper for Lua, as it is not heavily supported. The Lua wrapping libraries that provided the best C++ integration at the time were sol \cite{Github:Rapptz:Sol} and luacppinterface \cite{Github:davidsiaw:luacppinterface} which provided competing wrappers for Lua. Both sol and luacppinterface provided tested interfaces for working with the Lua VM from C++, however sol provided more modern bindings around C++ semantics. Lua's syntax is very similar to other C-style languages and is commonly utilized in games world-wide showcasing the modern nature of Lua (\cref{req:modern}). Lua's configuration style is extremely simple to utilize and with the ability to override all operators, configurations can be declared programmatically and have similar semantics to Scala's DSL with slightly less syntax flexibility (\cref{req:configuration}). Lastly, Lua is the near best performing scripting language after JavaScript and provides a very simple interaction point thanks to wrapping libraries. 

\begin{listing}[b!]
\begin{minted}{Lua}
entry {
    title = "Tecgraf",
    org = "Computer Graphics Technology Group, PUC-Rio",
    url = "http://www.tecgraf.puc-rio.br/",
    contact = "Waldemar Celes",
    description = [[
        TeCGraf is Lua's birthplace,
        and the language has been used there since 1993.
        Currently, more than thirty programmers in TeCGraf use
        Lua regularly; they have written more than two hundred
        thousand lines of code, distributed among dozens of
        final products.]]
}
\end{minted}
\caption{Table used to describe project information for the Lua.org site \cite{Ierusalimschy:PIL}}
\label{lst:lua:project-example}
\end{listing}

After consideration of the four different scripting languages, it was dropped to a choice between Lua and Python. Boost.Python's long history of use coupled with existing Boost usage in the procsim project made Python an enticing option. In addition, Python's feature set is very complete for most of the use cases for the procsim scripting language. A large issue with Python was that Boost.Python proved to be non-trivial to cross-compile on multiple platforms and imposed a direct coupling to the Python runtime. Further, to have simple configurations the use of \mintinline{python}{dict} literals is required which provides less type guarantees than Lua's equivalent with tables coupled to tableshape \cite{Python:BuiltinTypes, Ierusalimschy:PIL, Github:leafto:tableshape}. Further, Python's garbage collection can become difficult to work with in a non-managed language as it was not built with embedding in mind. Lua's focus on embedded scripting interfaces; its wide-spread use in games and industrial software as a scripting interface; the simplicity to build the C-based JIT or interpreted VM; and data description capabilities drove the decision to utilize Lua for configuration. 

\subsection{Lua integration}

Once Lua was chosen as a scripting language, the task of integrating the language into the existing object model arose. As stated previously, there existed two main projects under consideration for wrapping C++ API into a scripting engine: luacppinterface \cite{Github:davidsiaw:luacppinterface} and sol \cite{Github:Rapptz:Sol}. When first attempting to implement a scripting interface, luacppinterface was used because it was simpler than sol. However, due to simplicity and the design of luacppinterface binding large \cxxinline{class} instances became overly complicated and migration to sol occurred. sol provided a C++11-based interface to wrap classes and functions and expose them to a Lua environment.

When working with sol, several bugs were found in the implementation of the wrapper. Of significant importance was that sol in its current state would not allow passing a Lua table to the constructor of a C++ class \cite{Github:Rapptz:Sol:74}. I raised the issue and with several weeks of remaining open, the author of the library never responded. However, another GitHub user by the name of ThePhD revived the project under sol2 (herein referred to as sol) \cite{Github:ThePhD:sol2}. sol was updated to include support for C++14 features and heavily relied on inline variadic template functions to provide extremely efficient zero-cost bindings to Lua in a clean elegant syntax \cite{Github:ThePhD:sol2:benchmarks, Github:ThePhD:sol2:cxx-in-lua}. In \cref{lst:lua:sol2-example:player,lst:lua:sol2-example:usage,,lst:lua:sol2-example:bindings} sol2's capabilities are shown to simply bind the \cxxinline{class player} into Lua and utilize it within a simple toy script. Binding class properties is provided by utilizing \cxxinline{sol::property(...)} and \cxxinline{sol::readonly(...)} allowing a pattern very similar to Java Bean properties \cite{Oracle:JavaTutorial:JavaBeans}. The flexibility of overriding the operations allows for the creation of new ``syntax-like'' changes that alter the traditional behaviour of Lua operations as done with procsim.scala.

\begin{listing}[hp]
\begin{minted}{C++}
class player {
public:
    int bullets;
    
    player(): player(3) 
    { }
    
    player(int ammo)
        : bullets(ammo), hp(10)
    { }
    
    bool shoot () {
        if (bullets < 1) 
            return false;
    
        --bullets;
        return true;
    }
    
    void set_hp(int value) {
        hp = value;
    }
    
    int get_hp() const {
        return hp;
    }
    
private:
    int hp;
};
\end{minted}
\caption{\cxxinline{class player} that holds two fields, one for hitpoints (\cxxinline{hp}) and one for bullets a player has (adapted from \cite{Github:ThePhD:sol2:cxx-in-lua}).}
\label{lst:lua:sol2-example:player}
\end{listing}

\begin{listing}[hp]
\begin{minted}{Lua}
-- player_script.lua

-- call single argument integer constructor
p1 = player.new(2)

-- p2 is still here from being
-- set with lua["p2"] = std::make_shared<player>(0);
-- in cpp file
local p2shoots = p2:shoot()
assert(not p2shoots) -- had 0 ammo

-- set variable property setter
p1.hp = 545;
-- get variable through property getter
print(p1.hp);

local did_shoot_1 = p1:shoot()
print(did_shoot_1)
print(p1.bullets)
local did_shoot_2 = p1:shoot()
print(did_shoot_2)
print(p1.bullets)
local did_shoot_3 = p1:shoot()
print(did_shoot_3)

-- can read
print(p1.bullets)
-- would error: is a readonly variable, cannot write
-- p1.bullets = 20
\end{minted}
\caption{Utilize the \cxxinline{class player} within a Lua script (adapted from \cite{Github:ThePhD:sol2:cxx-in-lua}).}
\label{lst:lua:sol2-example:usage}
\end{listing}

\begin{listing}[hp]
\begin{minted}{C++}
#include <sol.hpp>

int main () {
    sol::state lua;
    
    // Register usertype metatable
    lua.new_usertype<player>( "player",
        // 2 constructors
        sol::constructors<player(), player(int)>(),
        
        // member function that returns a variable
        "shoot", &player::shoot,
        
        // gets or set the value using member variable syntax
        "hp", sol::property(&player::get_hp, &player::set_hp),
        
        // can only read from, not write to
        "bullets", sol::readonly( &player::bullets )
    );
    
    // set a variable "p2" with a new "player" with 0 ammo
    // using an std::shared_ptr<>
    lua["p2"] = std::make_shared<player>(0);
    
    // run the example script
    lua.script_file("player_script.lua");
    
    const std::shared_ptr<player> p1 = lua["p1"];
    std::cout << p1->hp << std::endl; // prints 545
}
\end{minted}
\caption{Required bindings to allow utilization of \cxxinline{class player} from Lua (adapted from \cite{Github:ThePhD:sol2:cxx-in-lua}).}
\label{lst:lua:sol2-example:bindings}
\end{listing}

Working with sol was not entirely without problems. Because the library was new, this author worked closely with the new author of sol, ``ThePhD,'' to implement features and verify behaviours across multiple platforms (a requirement deeply discussed within \cref{sec:lua:sec:cross-platform-development}). This author implemented tests and benchmarks on the macOS platform as ``ThePhD'' did not have access to the platform. This author worked to add continuous integration support for sol through Travis-CI \cite{TravisCI} to confirm the libraries support on platforms required for the procsim project \cite{Github:ThePhD:sol2:pr:17, Github:ThePhD:sol2:pr:18}. The contributions surrounding continuous integration improvements were re-purposed configurations from procsim provided as open-source contribution to the sol project. Within the procsim project, the development heavily tracked the cutting-edge of sol development. The project was frequently wrought with software bugs that were reported or fixed by this author. 

\subsection{Configuration versus Simulation entities}

The original hc12sim project shared all components between both configuration and simulation. This meant less lines of code to understand and generally made the software ``easier'' to write. However, as modules grow in size, they become more complicated to maintain. Something found in work for \cref{ch:scala-akka} was that immutable objects produce easier to reason and better performing software. When beginning to create bindings for the simulation entities, it became increasingly obvious that a lot of the information provided to users to reduce the cognitive load needed to specify a configuration was not required at runtime of a simulation. These values also needed to be configurable within the scripting interface but, once set, could be immutable within a simulation context.

For example, when defining a \verb|Register| component, there are a multitude of values that are used when configuring an instance: 
\begin{itemize}
\item name - A logical name
\item clock - The clock that this Register is bound to (Registers are sequential)
\item readCount - Number of clock ticks this takes to read a value (required to ``slow'' down execution)
\item writeCount - Number of clock ticks to write a value to the Register
\item access - What type of access this register supports
\end{itemize}
When writing a configuration, the it is likely easier to define a \verb|clock| by its name or by a literal definition. At simulation time, the clock name does not matter, but having a reference to it matters. Further, the name of any component is useful to students but it provides no use to the simulation itself. This idea of ``higher-level configuration'' that is transformed into a ``lower-level'' representation directly mimics a compiler's approach when compiling high-level code to the compiler's intermediate representation. The intermediate representation removes superfluous information in exchange for a smaller but easier to optimize representation of the programs state which is how simulation entities should be represented.

% We removed half of these parameters from the ts version -- they have no use when you are treating everything
% as I/O connections, they just aren't necessary. Want a read-only register? Don't connect something to D.


\section{Lua-based Configuration}

To limit the scope of a system's configuration, procsim only exposes the following components: 
\begin{itemize}
    \item Registers,
    \item Memory banks (ROM, RAM),
    \item Clocks,
    \item and Arithmetic operations
\end{itemize}
The simplification of the model allows for reduction of complexity in an attempt to reduce the cognitive load for students when creating their own processors, similar to the approach taken by \cite{Skrien2001,Garcia2009}. 

\begin{listing}[hp!]
    \inputminted{lua}{./listings/x86_example.1.lua}
    \caption{Sample configuration showcasing a simple x86-like processor}
    \label{lst:lua:x86-example-1}
\end{listing}

\begin{listing*}[hp!]
    \inputminted[firstnumber=last]{lua}{./listings/x86_example.2.lua}
    \caption{(Continued) Sample configuration showcasing a simple x86-like processor}
    \label{lst:lua:x86-example-2}
\end{listing*}

\begin{listing*}[hp!]
    \inputminted[firstnumber=last]{lua}{./listings/x86_example.3.lua}
    \caption{(Continued) Sample configuration showcasing a simple x86-like processor}
    \label{lst:lua:x86-example-3}
\end{listing*}

\lipsum[2]

\section{Side-effect-based execution}
\lipsum[4]

\section{Technology challenges}
\lipsum[3]

\section{``Lessons learned''}
\lipsum[1]
